{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pandas import DataFrame, Series\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Functions\n",
    "\n",
    "def post_to_words( raw_post ):\n",
    "    \"\"\"Function to convert a raw document to a string of words.\n",
    "    \n",
    "    Inputs : \n",
    "    \n",
    "    - raw_post : a single string \n",
    "    \n",
    "    Output :\n",
    "    \n",
    "    - a single string containing a preprocessed document\"\"\"\n",
    "    \n",
    "    # 1. Remove code\n",
    "    \n",
    "    liste = raw_post.split('code>')\n",
    "    \n",
    "    liste_clean = []\n",
    "    for i in range(0,len(liste),2):\n",
    "        elt = liste[i]\n",
    "        liste_clean.append(elt) \n",
    "        \n",
    "    string_clean = \" \".join(liste_clean)\n",
    "    \n",
    "    # 2. Remove HTML\n",
    "    post_text = BeautifulSoup(string_clean).get_text() \n",
    "    #\n",
    "    # 3. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^#+a-zA-Z]\", \" \", post_text) \n",
    "    #\n",
    "    # 4. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 5. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 6. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 7. Lematize\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    lems = []\n",
    "\n",
    "    for word in meaningful_words:\n",
    "        word_clean = lemmatizer.lemmatize(word)\n",
    "        lems.append(word_clean)\n",
    "    #\n",
    "    # 8. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( lems ))\n",
    "\n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    \n",
    "    \"\"\"Function defining personalized tokenizer for sklearn's CountVectorizer in order to keep the created \n",
    "    bigrams in tag's format.\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    - doc : string to be tokenized\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    - tokenized string\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    tokens = doc.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "stop_words = [\"i'm\", 'would', '1', '0', '2', \"i've\", 'could', 'anyone', 'also', '3', 'thanks', \n",
    "               'two', 'however', \"i'd\", '5', \"+\", \"#\", \"im\", \"ive\", \"dont\", \"cant\", \"id\", \")\", \"(\", 'code','using','file','error','get',\n",
    " 'like','using', 'get', 'like', 'want', 'use', 'work', 'one', 'trying', 'need', 'way', 'tried',\n",
    " 'problem', 'following', 'run', 'example', 'help', 'new', 'know', 'working','make', 'create','first',\n",
    " 'issue', 'find', 'see', 'different', 'show', 'return', 'test', 'question', 'getting', 'something',\n",
    " 'try', 'able', 'e', 'another', 'used','without', 'look', 'please', 'possible', 'x', 'found','fine',\n",
    " 'created', 'case', 'would-like', 'still', 'inside','wrong','right', 'give', 'seems', 'cannot',\n",
    " 'idea', 'instead', 'sure', 'b', 'every', 'react', 'based', 'simple', 'got', 'v', 'already', 'look-like',\n",
    " 'many', 'called', 'say', 'correct', 'main','specific','understand', 'added', 'since', 'currently',\n",
    " 'back', 'current']\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "tf_idf_final = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = my_tokenizer,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             strip_accents=None,\n",
    "                             max_features = None, \n",
    "                             lowercase=False,\n",
    "                             stop_words = stop_words) \n",
    "\n",
    "top_tags = np.load('Data/top_tags.npy')\n",
    "\n",
    "def reduce_tags(y):\n",
    "    \n",
    "    #new_tags = []\n",
    "    \n",
    "    tag = y.split()\n",
    "    tag = [word for word in tag if word in top_tags]\n",
    "    \n",
    "        \n",
    "    #new_tags.append(tag)\n",
    "        \n",
    "    return \" \".join(tag)\n",
    "\n",
    "def cleaning_target(raw_target):\n",
    "    \"\"\"Function to remove '<>' signs from target list and to replace them with a space\n",
    "    \n",
    "    Arguments :\n",
    "    - raw_target : a Series of tags\n",
    "    \n",
    "    Return :\n",
    "    - Series with cleaned tags\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    #Remove first '<'\n",
    "    tag = raw_target[1:]\n",
    "    \n",
    "    #Remove last '>'\n",
    "    tag = tag[:-1]\n",
    "    \n",
    "    #Remove remaining '><' signs\n",
    "    tag = tag.split('><')\n",
    "    \n",
    "    #Converting back to string\n",
    "    tag_clean = \" \".join(tag)  \n",
    "    \n",
    "    return tag_clean\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "tag_vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = my_tokenizer,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             strip_accents=None,\n",
    "                             max_features=None,\n",
    "                             lowercase=False,\n",
    "                             stop_words = stop_words) \n",
    "\n",
    "model_final = joblib.load('model_final.plk')\n",
    "\n",
    "\n",
    "# X train cleaning & formatting\n",
    "X_train = pd.read_csv('Data/X_train.csv', sep='\\t')\n",
    "X_train['body_clean']=X_train['body'].apply(lambda x: post_to_words(x))\n",
    "X_train['title_clean']=X_train['title'].apply(lambda x: post_to_words(x))\n",
    "X_train['post_w'] = (X_train['title_clean'] + \" \") *3 + \" \" + X_train['body_clean']\n",
    "X_train_final = tf_idf_final.fit_transform(X_train['post_w'])\n",
    "X_train_final = X_train_final.toarray()\n",
    "\n",
    "# y train cleaning and formatting\n",
    "y_train = pd.read_csv('Data/y_train.csv', sep='\\t', header=None)\n",
    "y_train = y_train[1]\n",
    "y_train_clean = y_train.apply(lambda x: cleaning_target(x)) \n",
    "y_train_top_tags = y_train_clean.apply(lambda x : reduce_tags(x))\n",
    "y_train_final = tag_vectorizer.fit_transform(y_train_top_tags)\n",
    "y_train_final = y_train_final.toarray()\n",
    "y_train_final_vocab = tag_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saisir le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Image classification using CNN\n"
     ]
    }
   ],
   "source": [
    "# User's title input\n",
    "title = input(\"Title: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body: How could I implement CNN with Keras ?\n"
     ]
    }
   ],
   "source": [
    "# User's body input\n",
    "body = input(\"Body: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retourner les tags prédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras\n"
     ]
    }
   ],
   "source": [
    "# Cleaning\n",
    "body_clean = post_to_words(body)\n",
    "title_clean = post_to_words(title)\n",
    "post_w = (title_clean + \" \") *3 + \" \" + body_clean\n",
    "post_w = Series(post_w)\n",
    "\n",
    "# Formatting\n",
    "new_question_final = tf_idf_final.transform(post_w)\n",
    "new_question_final = new_question_final.toarray()\n",
    "\n",
    "# Fit the sample\n",
    "predicted_tags_new_quest = model_final.predict(new_question_final)\n",
    "\n",
    "# Print the tags\n",
    "for freq, word in zip(predicted_tags_new_quest[0], y_train_final_vocab):\n",
    "    if freq > 0:\n",
    "        print(word) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
